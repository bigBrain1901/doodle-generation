{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"tDybPQiEFQuJ"},"cell_type":"markdown","source":["In this notebook, we will show how to load pre-trained models and draw things with sketch-rnn"]},{"metadata":{"id":"D7ObpAUh9jrk","executionInfo":{"status":"ok","timestamp":1711999006197,"user_tz":420,"elapsed":15159,"user":{"displayName":"Steven Lu","userId":"12513680598435341814"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"59196f24-c1a9-48ba-bb10-b2bfe36607b1"},"cell_type":"code","source":["!pip install -qU svgwrite"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"metadata":{"id":"LebxcF4p90OR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711999027115,"user_tz":420,"elapsed":17743,"user":{"displayName":"Steven Lu","userId":"12513680598435341814"}},"outputId":"cf07e088-7c29-48b8-c139-6355b463f355"},"cell_type":"code","source":["!pip install -q magenta"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.1/210.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.5/204.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}]},{"metadata":{"id":"k0GqvYgB9JLC"},"cell_type":"code","source":["# import the required libraries\n","import numpy as np\n","import time\n","import random\n","import pickle\n","import codecs\n","import collections\n","import os\n","import math\n","import json\n","import tensorflow as tf\n","from six.moves import xrange"],"execution_count":null,"outputs":[]},{"metadata":{"id":"UI4ZC__4FQuL"},"cell_type":"code","source":["# libraries required for visualisation:\n","from IPython.display import SVG, display\n","import PIL\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# set numpy output to something sensible\n","np.set_printoptions(precision=8, edgeitems=6, linewidth=200, suppress=True)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"4xYY-TUd9aiD"},"cell_type":"code","source":["import svgwrite # conda install -c omnia svgwrite=1.1.6"],"execution_count":null,"outputs":[]},{"metadata":{"id":"NkFS0E1zFQuU","colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"status":"error","timestamp":1711766613064,"user_tz":420,"elapsed":8,"user":{"displayName":"Ishaan Singh","userId":"14388252339264767661"}},"outputId":"2953d7d6-90e2-4baf-80c2-367362bf6dd1"},"cell_type":"code","source":["# import our command line tools\n","from magenta.models.sketch_rnn.sketch_rnn_train import *\n","from magenta.models.sketch_rnn.model import *\n","from magenta.models.sketch_rnn.utils import *\n","from magenta.models.sketch_rnn.rnn import *"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'magenta'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-8c151ed9ec1f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import our command line tools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmagenta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msketch_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msketch_rnn_train\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmagenta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msketch_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmagenta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msketch_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmagenta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msketch_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'magenta'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"metadata":{"id":"GBde4xkEFQuX"},"cell_type":"code","source":["# little function that displays vector images and saves them to .svg\n","def draw_strokes(data, factor=0.2, svg_filename = '/tmp/sketch_rnn/svg/sample.svg'):\n","  tf.gfile.MakeDirs(os.path.dirname(svg_filename))\n","  min_x, max_x, min_y, max_y = get_bounds(data, factor)\n","  dims = (50 + max_x - min_x, 50 + max_y - min_y)\n","  dwg = svgwrite.Drawing(svg_filename, size=dims)\n","  dwg.add(dwg.rect(insert=(0, 0), size=dims,fill='white'))\n","  lift_pen = 1\n","  abs_x = 25 - min_x\n","  abs_y = 25 - min_y\n","  p = \"M%s,%s \" % (abs_x, abs_y)\n","  command = \"m\"\n","  for i in xrange(len(data)):\n","    if (lift_pen == 1):\n","      command = \"m\"\n","    elif (command != \"l\"):\n","      command = \"l\"\n","    else:\n","      command = \"\"\n","    x = float(data[i,0])/factor\n","    y = float(data[i,1])/factor\n","    lift_pen = data[i, 2]\n","    p += command+str(x)+\",\"+str(y)+\" \"\n","  the_color = \"black\"\n","  stroke_width = 1\n","  dwg.add(dwg.path(p).stroke(the_color,stroke_width).fill(\"none\"))\n","  dwg.save()\n","  display(SVG(dwg.tostring()))\n","\n","# generate a 2D grid of many vector drawings\n","def make_grid_svg(s_list, grid_space=10.0, grid_space_x=16.0):\n","  def get_start_and_end(x):\n","    x = np.array(x)\n","    x = x[:, 0:2]\n","    x_start = x[0]\n","    x_end = x.sum(axis=0)\n","    x = x.cumsum(axis=0)\n","    x_max = x.max(axis=0)\n","    x_min = x.min(axis=0)\n","    center_loc = (x_max+x_min)*0.5\n","    return x_start-center_loc, x_end\n","  x_pos = 0.0\n","  y_pos = 0.0\n","  result = [[x_pos, y_pos, 1]]\n","  for sample in s_list:\n","    s = sample[0]\n","    grid_loc = sample[1]\n","    grid_y = grid_loc[0]*grid_space+grid_space*0.5\n","    grid_x = grid_loc[1]*grid_space_x+grid_space_x*0.5\n","    start_loc, delta_pos = get_start_and_end(s)\n","\n","    loc_x = start_loc[0]\n","    loc_y = start_loc[1]\n","    new_x_pos = grid_x+loc_x\n","    new_y_pos = grid_y+loc_y\n","    result.append([new_x_pos-x_pos, new_y_pos-y_pos, 0])\n","\n","    result += s.tolist()\n","    result[-1][2] = 1\n","    x_pos = new_x_pos+delta_pos[0]\n","    y_pos = new_y_pos+delta_pos[1]\n","  return np.array(result)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"if7-UyxzFQuY"},"cell_type":"markdown","source":["define the path of the model you want to load, and also the path of the dataset"]},{"metadata":{"id":"Dipv1EbsFQuZ"},"cell_type":"code","source":["data_dir = 'http://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep/'\n","models_root_dir = '/tmp/sketch_rnn/models'\n","model_dir = '/tmp/sketch_rnn/models/aaron_sheep/layer_norm'"],"execution_count":null,"outputs":[]},{"metadata":{"id":"eaSqI0fIFQub"},"cell_type":"code","source":["download_pretrained_models(models_root_dir=models_root_dir)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"G4sRuxyn_1aO"},"cell_type":"code","source":["def load_env_compatible(data_dir, model_dir):\n","  \"\"\"Loads environment for inference mode, used in jupyter notebook.\"\"\"\n","  # modified https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_rnn/sketch_rnn_train.py\n","  # to work with depreciated tf.HParams functionality\n","  model_params = sketch_rnn_model.get_default_hparams()\n","  with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n","    data = json.load(f)\n","  fix_list = ['conditional', 'is_training', 'use_input_dropout', 'use_output_dropout', 'use_recurrent_dropout']\n","  for fix in fix_list:\n","    data[fix] = (data[fix] == 1)\n","  model_params.parse_json(json.dumps(data))\n","  return load_dataset(data_dir, model_params, inference_mode=True)\n","\n","def load_model_compatible(model_dir):\n","  \"\"\"Loads model for inference mode, used in jupyter notebook.\"\"\"\n","  # modified https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_rnn/sketch_rnn_train.py\n","  # to work with depreciated tf.HParams functionality\n","  model_params = sketch_rnn_model.get_default_hparams()\n","  with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n","    data = json.load(f)\n","  fix_list = ['conditional', 'is_training', 'use_input_dropout', 'use_output_dropout', 'use_recurrent_dropout']\n","  for fix in fix_list:\n","    data[fix] = (data[fix] == 1)\n","  model_params.parse_json(json.dumps(data))\n","\n","  model_params.batch_size = 1  # only sample one at a time\n","  eval_model_params = sketch_rnn_model.copy_hparams(model_params)\n","  eval_model_params.use_input_dropout = 0\n","  eval_model_params.use_recurrent_dropout = 0\n","  eval_model_params.use_output_dropout = 0\n","  eval_model_params.is_training = 0\n","  sample_model_params = sketch_rnn_model.copy_hparams(eval_model_params)\n","  sample_model_params.max_seq_len = 1  # sample one point at a time\n","  return [model_params, eval_model_params, sample_model_params]"],"execution_count":null,"outputs":[]},{"metadata":{"id":"9m-jSAb3FQuf"},"cell_type":"code","source":["[train_set, valid_set, test_set, hps_model, eval_hps_model, sample_hps_model] = load_env_compatible(data_dir, model_dir)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"1pHS8TSgFQui"},"cell_type":"code","source":["# construct the sketch-rnn model here:\n","reset_graph()\n","model = Model(hps_model)\n","eval_model = Model(eval_hps_model, reuse=True)\n","sample_model = Model(sample_hps_model, reuse=True)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"1gxYLPTQFQuk"},"cell_type":"code","source":["sess = tf.InteractiveSession()\n","sess.run(tf.global_variables_initializer())"],"execution_count":null,"outputs":[]},{"metadata":{"id":"bVlDyfN_FQum"},"cell_type":"code","source":["# loads the weights from checkpoint into our model\n","load_checkpoint(sess, model_dir)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"EOblwpFeFQuq"},"cell_type":"markdown","source":["We define two convenience functions to encode a stroke into a latent vector, and decode from latent vector to stroke."]},{"metadata":{"id":"tMFlV487FQur"},"cell_type":"code","source":["def encode(input_strokes):\n","  strokes = to_big_strokes(input_strokes).tolist()\n","  strokes.insert(0, [0, 0, 1, 0, 0])\n","  seq_len = [len(input_strokes)]\n","  draw_strokes(to_normal_strokes(np.array(strokes)))\n","  return sess.run(eval_model.batch_z, feed_dict={eval_model.input_data: [strokes], eval_model.sequence_lengths: seq_len})[0]"],"execution_count":null,"outputs":[]},{"metadata":{"id":"1D5CV7ZlFQut"},"cell_type":"code","source":["def decode(z_input=None, draw_mode=True, temperature=0.1, factor=0.2):\n","  z = None\n","  if z_input is not None:\n","    z = [z_input]\n","  sample_strokes, m = sample(sess, sample_model, seq_len=eval_model.hps.max_seq_len, temperature=temperature, z=z)\n","  strokes = to_normal_strokes(sample_strokes)\n","  if draw_mode:\n","    draw_strokes(strokes, factor)\n","  return strokes"],"execution_count":null,"outputs":[]},{"metadata":{"id":"fUOAvRQtFQuw"},"cell_type":"code","source":["# get a sample drawing from the test set, and render it to .svg\n","stroke = test_set.random_sample()\n","draw_strokes(stroke)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"j114Re2JFQuz"},"cell_type":"markdown","source":["Let's try to encode the sample stroke into latent vector $z$"]},{"metadata":{"id":"DBRjPBo-FQu0"},"cell_type":"code","source":["z = encode(stroke)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"-37v6eZLFQu5"},"cell_type":"code","source":["_ = decode(z, temperature=0.8) # convert z back to drawing at temperature of 0.8"],"execution_count":null,"outputs":[]},{"metadata":{"id":"M5ft6IEBFQu9"},"cell_type":"markdown","source":["Create generated grid at various temperatures from 0.1 to 1.0"]},{"metadata":{"id":"BuhaZI0aFQu9"},"cell_type":"code","source":["stroke_list = []\n","for i in range(10):\n","  stroke_list.append([decode(z, draw_mode=False, temperature=0.1*i+0.1), [0, i]])\n","stroke_grid = make_grid_svg(stroke_list)\n","draw_strokes(stroke_grid)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"4xiwp3_DFQvB"},"cell_type":"markdown","source":["Latent Space Interpolation Example between $z_0$ and $z_1$"]},{"metadata":{"id":"WSX0uvZTFQvD"},"cell_type":"code","source":["# get a sample drawing from the test set, and render it to .svg\n","z0 = z\n","_ = decode(z0)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"jQf99TxOFQvH"},"cell_type":"code","source":["stroke = test_set.random_sample()\n","z1 = encode(stroke)\n","_ = decode(z1)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"tDqJR8_eFQvK"},"cell_type":"markdown","source":["Now we interpolate between sheep $z_0$ and sheep $z_1$"]},{"metadata":{"id":"_YkPNL5SFQvL"},"cell_type":"code","source":["z_list = [] # interpolate spherically between z0 and z1\n","N = 10\n","for t in np.linspace(0, 1, N):\n","  z_list.append(slerp(z0, z1, t))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"UoM-W1tQFQvM"},"cell_type":"code","source":["# for every latent vector in z_list, sample a vector image\n","reconstructions = []\n","for i in range(N):\n","  reconstructions.append([decode(z_list[i], draw_mode=False), [0, i]])"],"execution_count":null,"outputs":[]},{"metadata":{"id":"mTqmlL6GFQvQ"},"cell_type":"code","source":["stroke_grid = make_grid_svg(reconstructions)\n","draw_strokes(stroke_grid)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"vFwPna6uFQvS"},"cell_type":"markdown","source":["Let's load the Flamingo Model, and try Unconditional (Decoder-Only) Generation"]},{"metadata":{"id":"HH-YclgNFQvT"},"cell_type":"code","source":["model_dir = '/tmp/sketch_rnn/models/flamingo/lstm_uncond'"],"execution_count":null,"outputs":[]},{"metadata":{"id":"-Znvy3KxFQvU"},"cell_type":"code","source":["[hps_model, eval_hps_model, sample_hps_model] = load_model_compatible(model_dir)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"cqDNK1cYFQvZ"},"cell_type":"code","source":["# construct the sketch-rnn model here:\n","reset_graph()\n","model = Model(hps_model)\n","eval_model = Model(eval_hps_model, reuse=True)\n","sample_model = Model(sample_hps_model, reuse=True)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"7wzerSI6FQvd"},"cell_type":"code","source":["sess = tf.InteractiveSession()\n","sess.run(tf.global_variables_initializer())"],"execution_count":null,"outputs":[]},{"metadata":{"id":"6mzk8KjOFQvf"},"cell_type":"code","source":["# loads the weights from checkpoint into our model\n","load_checkpoint(sess, model_dir)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"X88CgcyuFQvh"},"cell_type":"code","source":["# randomly unconditionally generate 10 examples\n","N = 10\n","reconstructions = []\n","for i in range(N):\n","  reconstructions.append([decode(temperature=0.5, draw_mode=False), [0, i]])"],"execution_count":null,"outputs":[]},{"metadata":{"id":"k57REtd_FQvj"},"cell_type":"code","source":["stroke_grid = make_grid_svg(reconstructions)\n","draw_strokes(stroke_grid)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"L-rJ0iUQFQvl"},"cell_type":"markdown","source":["Let's load the owl model, and generate two sketches using two random IID gaussian latent vectors"]},{"metadata":{"id":"of4SWwGdFQvm"},"cell_type":"code","source":["model_dir = '/tmp/sketch_rnn/models/owl/lstm'"],"execution_count":null,"outputs":[]},{"metadata":{"id":"jJiSZFQeFQvp"},"cell_type":"code","source":["[hps_model, eval_hps_model, sample_hps_model] = load_model_compatible(model_dir)\n","# construct the sketch-rnn model here:\n","reset_graph()\n","model = Model(hps_model)\n","eval_model = Model(eval_hps_model, reuse=True)\n","sample_model = Model(sample_hps_model, reuse=True)\n","sess = tf.InteractiveSession()\n","sess.run(tf.global_variables_initializer())\n","# loads the weights from checkpoint into our model\n","load_checkpoint(sess, model_dir)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"vR4TDoi5FQvr"},"cell_type":"code","source":["z_0 = np.random.randn(eval_model.hps.z_size)\n","_ = decode(z_0)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"ZX23lTnpFQvt"},"cell_type":"code","source":["z_1 = np.random.randn(eval_model.hps.z_size)\n","_ = decode(z_1)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"7FjQsF_2FQvv"},"cell_type":"markdown","source":["Let's interpolate between the two owls $z_0$ and $z_1$"]},{"metadata":{"id":"u6G37E8_FQvw"},"cell_type":"code","source":["z_list = [] # interpolate spherically between z_0 and z_1\n","N = 10\n","for t in np.linspace(0, 1, N):\n","  z_list.append(slerp(z_0, z_1, t))\n","# for every latent vector in z_list, sample a vector image\n","reconstructions = []\n","for i in range(N):\n","  reconstructions.append([decode(z_list[i], draw_mode=False, temperature=0.1), [0, i]])"],"execution_count":null,"outputs":[]},{"metadata":{"id":"OULjMktmFQvx"},"cell_type":"code","source":["stroke_grid = make_grid_svg(reconstructions)\n","draw_strokes(stroke_grid)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"OiXNC-YsFQv0"},"cell_type":"markdown","source":["Let's load the model trained on both cats and buses!  catbus!"]},{"metadata":{"id":"SL7WpDDQFQv0"},"cell_type":"code","source":["model_dir = '/tmp/sketch_rnn/models/catbus/lstm'"],"execution_count":null,"outputs":[]},{"metadata":{"id":"Cvk5WOqHFQv2"},"cell_type":"code","source":["[hps_model, eval_hps_model, sample_hps_model] = load_model_compatible(model_dir)\n","# construct the sketch-rnn model here:\n","reset_graph()\n","model = Model(hps_model)\n","eval_model = Model(eval_hps_model, reuse=True)\n","sample_model = Model(sample_hps_model, reuse=True)\n","sess = tf.InteractiveSession()\n","sess.run(tf.global_variables_initializer())\n","# loads the weights from checkpoint into our model\n","load_checkpoint(sess, model_dir)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"icvlBPVkFQv5"},"cell_type":"code","source":["z_1 = np.random.randn(eval_model.hps.z_size)\n","_ = decode(z_1)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"uaNxd0LuFQv-"},"cell_type":"code","source":["z_0 = np.random.randn(eval_model.hps.z_size)\n","_ = decode(z_0)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"VtSYkS6mFQwC"},"cell_type":"markdown","source":["Let's interpolate between a cat and a bus!!!"]},{"metadata":{"id":"qIDYUxBEFQwD"},"cell_type":"code","source":["z_list = [] # interpolate spherically between z_1 and z_0\n","N = 10\n","for t in np.linspace(0, 1, N):\n","  z_list.append(slerp(z_1, z_0, t))\n","# for every latent vector in z_list, sample a vector image\n","reconstructions = []\n","for i in range(N):\n","  reconstructions.append([decode(z_list[i], draw_mode=False, temperature=0.15), [0, i]])"],"execution_count":null,"outputs":[]},{"metadata":{"id":"ZHmnSjSaFQwH"},"cell_type":"code","source":["stroke_grid = make_grid_svg(reconstructions)\n","draw_strokes(stroke_grid)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"flZ_OgzCFQwJ"},"cell_type":"markdown","source":["Why stop here? Let's load the model trained on both elephants and pigs!!!"]},{"metadata":{"id":"S8WwK8FPFQwK"},"cell_type":"code","source":["model_dir = '/tmp/sketch_rnn/models/elephantpig/lstm'"],"execution_count":null,"outputs":[]},{"metadata":{"id":"meOH4AFXFQwM"},"cell_type":"code","source":["[hps_model, eval_hps_model, sample_hps_model] = load_model_compatible(model_dir)\n","# construct the sketch-rnn model here:\n","reset_graph()\n","model = Model(hps_model)\n","eval_model = Model(eval_hps_model, reuse=True)\n","sample_model = Model(sample_hps_model, reuse=True)\n","sess = tf.InteractiveSession()\n","sess.run(tf.global_variables_initializer())\n","# loads the weights from checkpoint into our model\n","load_checkpoint(sess, model_dir)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"foZiiYPdFQwO"},"cell_type":"code","source":["z_0 = np.random.randn(eval_model.hps.z_size)\n","_ = decode(z_0)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"6Gaz3QG1FQwS"},"cell_type":"code","source":["z_1 = np.random.randn(eval_model.hps.z_size)\n","_ = decode(z_1)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"oVtr7NnGFQwU"},"cell_type":"markdown","source":["Tribute to an episode of [South Park](https://en.wikipedia.org/wiki/An_Elephant_Makes_Love_to_a_Pig): The interpolation between an Elephant and a Pig"]},{"metadata":{"id":"lJs9JbROFQwU"},"cell_type":"code","source":["z_list = [] # interpolate spherically between z_1 and z_0\n","N = 10\n","for t in np.linspace(0, 1, N):\n","  z_list.append(slerp(z_0, z_1, t))\n","# for every latent vector in z_list, sample a vector image\n","reconstructions = []\n","for i in range(N):\n","  reconstructions.append([decode(z_list[i], draw_mode=False, temperature=0.15), [0, i]])"],"execution_count":null,"outputs":[]},{"metadata":{"id":"0FOuNfJMFQwW"},"cell_type":"code","source":["stroke_grid = make_grid_svg(reconstructions, grid_space_x=25.0)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"bZ6zpdiMFQwX"},"cell_type":"code","source":["draw_strokes(stroke_grid, factor=0.3)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"KUgVRGnSFQwa"},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}]}